{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HBcompare_code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "remLVL7udJ6n",
        "PmG5G1TgdTak",
        "_FeAr2MvutWR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOeyzzhd6mVa"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/Graph_based_methods/HBcompare/')\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remLVL7udJ6n"
      },
      "source": [
        "### Util.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgqBNOvTeA6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73cfb715-e5d7-4dc0-c773-a9f6c004fff2"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.sparse import isspmatrix\n",
        "\n",
        "\"\"\"Adapted from https://github.com/weihua916/powerful-gnns/blob/master/util.py\"\"\"\n",
        "\n",
        "class S2VGraph(object):\n",
        "    def __init__(self, g, label, node_tags=None, node_features=None):\n",
        "        '''\n",
        "            g: a networkx graph\n",
        "            label: an integer graph label\n",
        "            node_tags: a list of integer node tags\n",
        "            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n",
        "            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n",
        "            neighbors: list of neighbors (without self-loop)\n",
        "        '''\n",
        "        self.label = label\n",
        "        self.g = g\n",
        "        self.node_tags = node_tags\n",
        "        self.neighbors = []\n",
        "        self.node_features = 0\n",
        "        self.edge_mat = 0\n",
        "        self.max_neighbor = 0\n",
        "\n",
        "\n",
        "def load_data(dataset, degree_as_tag):\n",
        "    '''\n",
        "        dataset: name of dataset\n",
        "        test_proportion: ratio of test train split\n",
        "        seed: random seed for random splitting of dataset\n",
        "    '''\n",
        "\n",
        "    print('loading data')\n",
        "    g_list = []\n",
        "    label_dict = {}\n",
        "    feat_dict = {}\n",
        "\n",
        "    with open('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:\n",
        "        n_g = int(f.readline().strip())\n",
        "        for i in range(n_g):\n",
        "            row = f.readline().strip().split()\n",
        "            n, l = [int(w) for w in row]\n",
        "            if not l in label_dict:\n",
        "                mapped = len(label_dict)\n",
        "                label_dict[l] = mapped\n",
        "            g = nx.Graph()\n",
        "            node_tags = []\n",
        "            node_features = []\n",
        "            n_edges = 0\n",
        "            for j in range(n):\n",
        "                g.add_node(j)\n",
        "                row = f.readline().strip().split()\n",
        "                tmp = int(row[1]) + 2\n",
        "                if tmp == len(row):\n",
        "                    # no node attributes\n",
        "                    row = [int(w) for w in row]\n",
        "                    attr = None\n",
        "                else:\n",
        "                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])\n",
        "                if not row[0] in feat_dict:\n",
        "                    mapped = len(feat_dict)\n",
        "                    feat_dict[row[0]] = mapped\n",
        "                node_tags.append(feat_dict[row[0]])\n",
        "\n",
        "                if tmp > len(row):\n",
        "                    node_features.append(attr)\n",
        "\n",
        "                n_edges += row[1]\n",
        "                for k in range(2, len(row)):\n",
        "                    g.add_edge(j, row[k])\n",
        "\n",
        "            if node_features != []:\n",
        "                node_features = np.stack(node_features)\n",
        "                node_feature_flag = True\n",
        "            else:\n",
        "                node_features = None\n",
        "                node_feature_flag = False\n",
        "\n",
        "            assert len(g) == n\n",
        "\n",
        "            g_list.append(S2VGraph(g, l, node_tags))\n",
        "     \n",
        "\n",
        "    #add labels and edge_mat       \n",
        "    for g in g_list:\n",
        "        g.neighbors = [[] for i in range(len(g.g))]\n",
        "        for i, j in g.g.edges():\n",
        "            g.neighbors[i].append(j)\n",
        "            g.neighbors[j].append(i)\n",
        "        degree_list = []\n",
        "        for i in range(len(g.g)):\n",
        "            g.neighbors[i] = g.neighbors[i]\n",
        "            degree_list.append(len(g.neighbors[i]))\n",
        "        g.max_neighbor = max(degree_list)\n",
        "\n",
        "        g.label = label_dict[g.label]\n",
        "\n",
        "        edges = [list(pair) for pair in g.g.edges()]\n",
        "        edges.extend([[i, j] for j, i in edges])\n",
        "\n",
        "        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())\n",
        "\n",
        "        g.edge_mat = np.transpose(np.array(edges, dtype=np.int32), (1,0))\n",
        "\n",
        "    if degree_as_tag:\n",
        "        for g in g_list:\n",
        "            g.node_tags = list(dict(g.g.degree).values())\n",
        "\n",
        "    #Extracting unique tag labels   \n",
        "    tagset = set([])\n",
        "    for g in g_list:\n",
        "        tagset = tagset.union(set(g.node_tags))\n",
        "\n",
        "    tagset = list(tagset)\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\n",
        "\n",
        "    for g in g_list:\n",
        "        g.node_features = np.zeros((len(g.node_tags), len(tagset)), dtype=np.float32)\n",
        "        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1\n",
        "\n",
        "\n",
        "    print('# classes: %d' % len(label_dict))\n",
        "    print('# maximum node tag: %d' % len(tagset))\n",
        "\n",
        "    print(\"# data: %d\" % len(g_list))\n",
        "\n",
        "    return g_list, len(label_dict)\n",
        "\n",
        "\n",
        "def separate_data(graph_list, fold_idx, seed=0):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "    labels = [graph.label for graph in graph_list]\n",
        "    idx_list = []\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "\n",
        "    train_graph_list = [graph_list[i] for i in train_idx]\n",
        "    test_graph_list = [graph_list[i] for i in test_idx]\n",
        "\n",
        "    return train_graph_list, test_graph_list\n",
        "    \n",
        "\n",
        "\"\"\"Get indexes of train and test sets\"\"\"\n",
        "def separate_data_idx(graph_list, fold_idx, seed=0):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "    labels = [graph.label for graph in graph_list]\n",
        "    idx_list = []\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "\n",
        "    return train_idx, test_idx\n",
        "\n",
        "\"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "data_name = \"dataset5\"\n",
        "use_degree_as_tag = True\n",
        "\n",
        "graphs, num_classes = load_data(data_name, use_degree_as_tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data\n",
            "# classes: 2\n",
            "# maximum node tag: 26\n",
            "# data: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmG5G1TgdTak"
      },
      "source": [
        "### GCN_layer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOYJcC4rdWlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4f22ed-a71f-4336-fe0b-074f67df87ff"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "    Thomas N. Kipf, Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. ICLR.\n",
        "    Modified from https://github.com/tkipf/gcn/blob/master/gcn/layers.py\n",
        "'''\n",
        "\n",
        "def uniform(shape, scale=0.05, name=None):\n",
        "    \"\"\"Uniform init.\"\"\"\n",
        "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
        "    initial = tf.compat.v1.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def zeros(shape, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def ones(shape, name=None):\n",
        "    \"\"\"All ones.\"\"\"\n",
        "    initial = tf.ones(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "flags = tf.compat.v1.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.adj = placeholders['adj']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        if not self.featureless:\n",
        "            pre_sup = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "        else:\n",
        "            pre_sup = self.vars['weights']\n",
        "        output = dot(self.adj, pre_sup, sparse=True)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FeAr2MvutWR"
      },
      "source": [
        "### Model_unsup_gcn.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgNzN7G8uxYz"
      },
      "source": [
        "class GCN_graph_cls(object):\n",
        "    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, num_sampled, vocab_size):\n",
        "        # Placeholders for input, output\n",
        "        print(vocab_size)\n",
        "        self.Adj_block = tf.compat.v1.sparse_placeholder(tf.float32, [None, None], name=\"Adj_block\")\n",
        "        self.X_concat = tf.compat.v1.sparse_placeholder(tf.float32, [None, feature_dim_size], name=\"X_concat\")\n",
        "        self.num_features_nonzero = tf.compat.v1.placeholder(tf.int32, name=\"num_features_nonzero\")\n",
        "        self.dropout = tf.compat.v1.placeholder(tf.float32, name=\"dropout\")\n",
        "        self.input_y = tf.compat.v1.placeholder(tf.int32, [None, 1], name=\"input_y\")\n",
        "\n",
        "        self.placeholders = {\n",
        "            'adj': self.Adj_block,\n",
        "            'dropout': self.dropout,\n",
        "            'num_features_nonzero': self.num_features_nonzero\n",
        "        }\n",
        "\n",
        "        self.input = self.X_concat   # set hidden_size = feature_dim_size if not tuning sizes of hidden stacked layers\n",
        "        in_hidden_size = feature_dim_size\n",
        "\n",
        "        self.output_vectors = []\n",
        "        #Construct k GNN layers\n",
        "        for idx_layer in range(num_GNN_layers):\n",
        "            sparse_inputs = False\n",
        "            if idx_layer == 0:\n",
        "                sparse_inputs = True\n",
        "            gcn_gnn = GraphConvolution(input_dim=in_hidden_size,\n",
        "                                                  output_dim=hidden_size,\n",
        "                                                  placeholders=self.placeholders,\n",
        "                                                  act=tf.nn.relu,\n",
        "                                                  dropout=True,\n",
        "                                                  sparse_inputs=sparse_inputs)\n",
        "\n",
        "            in_hidden_size = hidden_size\n",
        "            \n",
        "            # run --> output --> input for next layer\n",
        "            self.input = gcn_gnn(self.input)\n",
        "\n",
        "            # print('shape = ', self.input.get_shape)\n",
        "            #\n",
        "            self.output_vectors.append(self.input)\n",
        "\n",
        "        self.output_vectors = tf.concat(self.output_vectors, axis=1)\n",
        "        self.output_vectors = tf.nn.dropout(self.output_vectors, 1-self.dropout)\n",
        "\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            self.embedding_matrix = glorot([vocab_size, hidden_size*num_GNN_layers], name='node_embeddings')\n",
        "            self.softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
        "\n",
        "        self.total_loss = tf.reduce_mean(\n",
        "            tf.nn.sampled_softmax_loss(weights=self.embedding_matrix, biases=self.softmax_biases,\n",
        "                                       inputs=self.output_vectors, labels=self.input_y, num_sampled=num_sampled, num_classes=vocab_size))\n",
        "\n",
        "        self.saver = tf.compat.v1.train.Saver(tf.global_variables(), max_to_keep=500)\n",
        "        tf.logging.info('Seting up the main structure')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUZOCPYKMOaw"
      },
      "source": [
        "### Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5nsCyUnMSGY"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def cal_acc_only(predicted, gt):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if(predicted[i] == gt[i]):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/len(predicted)\n",
        "\n",
        "  return acc\n",
        "\n",
        "def calculate_acc(predicted, gt):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if(predicted[i] == gt[i]):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/len(predicted)\n",
        "\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  lb.fit(gt)\n",
        "\n",
        "  gt_binary = lb.transform(gt)\n",
        "  predicted_binary = lb.transform(predicted)\n",
        "\n",
        "  auc = roc_auc_score(gt_binary, predicted_binary, average = 'macro')\n",
        "\n",
        "  A = classification_report(predicted, gt, digits = 4)\n",
        "  print(A)\n",
        "\n",
        "\n",
        "  return acc, auc    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zIFPXFbDn_N"
      },
      "source": [
        "### Train HBcompare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgC-JN2DqA2"
      },
      "source": [
        "#! /usr/bin/env python\n",
        "!pip install keract\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "tf.compat.v1.set_random_seed(123)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from scipy.sparse import coo_matrix\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "def FCN_classify(X_train, X_test, label_train, label_test, num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "  # model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, label_train, epochs = 166, verbose =0)\n",
        "\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  predicted_labales = np.argmax(predictions, axis = 1)\n",
        "\n",
        "  # print(' acc ', acc, ' auc ', auc) \n",
        "  return predicted_labales\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "parser = ArgumentParser(\"GCN_Unsup\", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')\n",
        "\n",
        "parser.add_argument(\"--run_folder\", default=\"../\", help=\"\")\n",
        "parser.add_argument(\"--dataset\", default=\"dataset5\", help=\"Name of the dataset.\")\n",
        "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"Learning rate\")\n",
        "parser.add_argument(\"--batch_size\", default = 4, type=int, help=\"Batch Size\")\n",
        "parser.add_argument(\"--num_epochs\", default = 50, type=int, help=\"Number of training epochs\")\n",
        "parser.add_argument(\"--saveStep\", default=1, type=int, help=\"\")\n",
        "parser.add_argument(\"--allow_soft_placement\", default=True, type=bool, help=\"Allow device soft device placement\")\n",
        "parser.add_argument(\"--log_device_placement\", default=False, type=bool, help=\"Log placement of ops on devices\")\n",
        "parser.add_argument(\"--model_name\", default='MUTAG', help=\"\")\n",
        "parser.add_argument(\"--dropout\", default=0.5, type=float, help=\"Dropout\")\n",
        "parser.add_argument(\"--num_GNN_layers\", default=6, type=int, help=\"Number of stacked layers\")\n",
        "parser.add_argument(\"--hidden_size\", default=64, type=int, help=\"size of hidden layers\")\n",
        "parser.add_argument('--num_sampled', default=512, type=int, help='')\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args = [])\n",
        "\n",
        "print(args)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "\n",
        "use_degree_as_tag = False\n",
        "\n",
        "if args.dataset == 'COLLAB' or args.dataset == 'IMDBBINARY' or args.dataset == 'IMDBMULTI':\n",
        "    use_degree_as_tag = True\n",
        "graphs, num_classes = load_data(args.dataset, use_degree_as_tag)\n",
        "feature_dim_size = graphs[0].node_features.shape[1]\n",
        "graph_labels = np.array([graph.label for graph in graphs])\n",
        "if \"REDDIT\" in args.dataset:\n",
        "    feature_dim_size = 4\n",
        "\n",
        "def get_Adj_matrix(batch_graph):\n",
        "    edge_mat_list = []\n",
        "    start_idx = [0]\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        start_idx.append(start_idx[i] + len(graph.g))\n",
        "        edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
        "\n",
        "    Adj_block_idx = np.concatenate(edge_mat_list, 1)\n",
        "    Adj_block_elem = np.ones(Adj_block_idx.shape[1])\n",
        "\n",
        "    #self-loop\n",
        "    num_node = start_idx[-1]\n",
        "    self_loop_edge = np.array([range(num_node), range(num_node)])\n",
        "    elem = np.ones(num_node)\n",
        "    Adj_block_idx = np.concatenate([Adj_block_idx, self_loop_edge], 1)\n",
        "    Adj_block_elem = np.concatenate([Adj_block_elem, elem], 0)\n",
        "\n",
        "    Adj_block = coo_matrix((Adj_block_elem, Adj_block_idx), shape=(num_node, num_node))\n",
        "\n",
        "    return Adj_block\n",
        "\n",
        "def get_graphpool(batch_graph):\n",
        "    start_idx = [0]\n",
        "    # compute the padded neighbor list\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        start_idx.append(start_idx[i] + len(graph.g))\n",
        "\n",
        "    idx = []\n",
        "    elem = []\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        elem.extend([1] * len(graph.g))\n",
        "        idx.extend([[i, j] for j in range(start_idx[i], start_idx[i + 1], 1)])\n",
        "\n",
        "    elem = np.array(elem)\n",
        "    idx = np.array(idx)\n",
        "\n",
        "    graph_pool = coo_matrix((elem, (idx[:, 0], idx[:, 1])), shape=(len(batch_graph), start_idx[-1]))\n",
        "    return graph_pool\n",
        "\n",
        "graph_pool = get_graphpool(graphs)\n",
        "\n",
        "def get_idx_nodes(selected_graph_idx):\n",
        "    idx_nodes = [np.where(graph_pool.getrow(i).toarray()[0] == 1)[0] for i in selected_graph_idx]\n",
        "    idx_nodes = np.reshape(np.concatenate(idx_nodes), (-1, 1))\n",
        "    return idx_nodes\n",
        "\n",
        "def get_batch_data(batch_graph):\n",
        "    # features\n",
        "    X_concat = np.concatenate([graph.node_features for graph in batch_graph], 0)\n",
        "    if \"REDDIT\" in args.dataset:\n",
        "        X_concat = np.tile(X_concat, feature_dim_size) #[1,1,1,1]\n",
        "        X_concat = X_concat * 0.01\n",
        "\n",
        "    X_concat = coo_matrix(X_concat)\n",
        "    X_concat = sparse_to_tuple(X_concat)\n",
        "    # adj\n",
        "    Adj_block = get_Adj_matrix(batch_graph)\n",
        "    Adj_block = sparse_to_tuple(Adj_block)\n",
        "\n",
        "    num_features_nonzero = X_concat[1].shape\n",
        "    return Adj_block, X_concat, num_features_nonzero\n",
        "\n",
        "class Batch_Loader(object):\n",
        "    def __call__(self):\n",
        "        selected_idx = np.random.permutation(len(graphs))[:args.batch_size]\n",
        "        batch_graph = [graphs[idx] for idx in selected_idx]\n",
        "        Adj_block, X_concat, num_features_nonzero = get_batch_data(batch_graph)\n",
        "        idx_nodes = get_idx_nodes(selected_idx)\n",
        "        return Adj_block, X_concat, num_features_nonzero, idx_nodes\n",
        "\n",
        "batch_nodes = Batch_Loader()\n",
        "\n",
        "print(\"Loading data... finished!\")\n",
        "# Training\n",
        "# ==================================================\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=args.allow_soft_placement, log_device_placement=args.log_device_placement)\n",
        "    session_conf.gpu_options.allow_growth = True\n",
        "    sess = tf.compat.v1.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        unsup_gcn = GCN_graph_cls(feature_dim_size=feature_dim_size,\n",
        "                      hidden_size=args.hidden_size,\n",
        "                      num_GNN_layers=args.num_GNN_layers,\n",
        "                      vocab_size=graph_pool.shape[1],\n",
        "                      num_sampled=args.num_sampled,\n",
        "                      )\n",
        "\n",
        "        # Define Training procedure\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "        grads_and_vars = optimizer.compute_gradients(unsup_gcn.total_loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        out_dir = os.path.abspath(os.path.join(args.run_folder, \"../runs_GCN_UnSup\", args.model_name))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "        def train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes):\n",
        "            feed_dict = {\n",
        "                unsup_gcn.Adj_block: Adj_block,\n",
        "                unsup_gcn.X_concat: X_concat,\n",
        "                unsup_gcn.num_features_nonzero: num_features_nonzero,\n",
        "                unsup_gcn.dropout: args.dropout,\n",
        "                unsup_gcn.input_y:idx_nodes\n",
        "            }\n",
        "            _, step, loss = sess.run([train_op, global_step, unsup_gcn.total_loss], feed_dict)\n",
        "            return loss\n",
        "\n",
        "        # write_acc = open(checkpoint_prefix + '_acc.txt', 'w')\n",
        "        max_acc = 0.0\n",
        "        idx_epoch = 0\n",
        "        num_batches_per_epoch = int((len(graphs) - 1) / args.batch_size) + 1\n",
        "        predicted_labels = []\n",
        "        acc_all = []\n",
        "        auc_all = []\n",
        "\n",
        "        for epoch in range(1, args.num_epochs+1):\n",
        "            loss = 0\n",
        "            for _ in range(num_batches_per_epoch):\n",
        "                Adj_block, X_concat, num_features_nonzero, idx_nodes = batch_nodes()\n",
        "                loss += train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes)\n",
        "                # current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
        "            print(loss)\n",
        "            # It will give tensor object\n",
        "            node_embeddings = graph.get_tensor_by_name('embedding/node_embeddings:0')\n",
        "            node_embeddings = sess.run(node_embeddings)\n",
        "            graph_embeddings = graph_pool.dot(node_embeddings)\n",
        "            #\n",
        "            acc_10folds = []\n",
        "            for fold_idx in range(0,5):\n",
        "                train_idx, test_idx = separate_data_idx(graphs, fold_idx)\n",
        "                train_graph_embeddings = graph_embeddings[train_idx]\n",
        "                test_graph_embeddings = graph_embeddings[test_idx]\n",
        "                train_labels = graph_labels[train_idx]\n",
        "                test_labels = graph_labels[test_idx]\n",
        "\n",
        "                cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "                cls.fit(train_graph_embeddings, train_labels)\n",
        "                # ACC = cls.score(test_graph_embeddings, test_labels)\n",
        "                predicted = cls.predict(test_graph_embeddings)\n",
        "                ACC = cal_acc_only(predicted, test_labels)\n",
        "\n",
        "\n",
        "                if(epoch == args.num_epochs):\n",
        "                  predicted = cls.predict(test_graph_embeddings)\n",
        "                  predicted_labels.append(predicted)\n",
        "\n",
        "                  acc, auc = calculate_acc(predicted, test_labels)\n",
        "\n",
        "                  auc_all.append(auc)\n",
        "                  acc_all.append(acc)\n",
        "\n",
        "                acc_10folds.append(ACC)\n",
        "                print('epoch ', epoch, ' fold ', fold_idx, ' acc ', ACC)\n",
        "\n",
        "            mean_10folds = statistics.mean(acc_10folds)\n",
        "            std_10folds = statistics.stdev(acc_10folds)\n",
        "            print('epoch ', epoch, ' mean: ', str(mean_10folds*100), ' std: ', str(std_10folds*100))\n",
        "\n",
        "\n",
        "print('acc_all', acc_all)   \n",
        "\n",
        "print('auc_all', auc_all)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}